{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "talented-dodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adequate-default",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (50000, 32, 32, 3)\n",
      "w1: (3072, 200)\n",
      "w2: (200, 10)\n",
      "b1: (200,)\n",
      "b2: (10,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "print('x_train:', x_train.shape)\n",
    "K = len(np.unique(y_train)) # Return the unique elements of a tratining output set and take it length as Classes\n",
    "Ntr = x_train.shape[0] # number of training examples\n",
    "Nte = x_test.shape[0] # number of testing examples\n",
    "Din = 3072 # By CIFAR10 data set with 32 x 32 x 3 color images\n",
    "\n",
    "x_train = x_train[range(Ntr), :]\n",
    "x_test = x_test[range(Nte), :]\n",
    "y_train = y_train[range(Ntr)]\n",
    "y_test = y_test[range(Nte)]\n",
    "\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=K) # This function returns a matrix of binary values (either ‘1’ or ‘0’). It has number of rows equal to the length of the input vector and number of columns equal to the number of classes.\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=K)\n",
    "x_train = np.reshape(x_train,(Ntr,Din)).astype('float32') # reshape the data set\n",
    "x_test = np.reshape(x_test,(Nte,Din)).astype('float32')\n",
    "\n",
    "#x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "mean_image = np.mean(x_train, axis=0)\n",
    "x_train = x_train - mean_image\n",
    "x_test = x_test - mean_image\n",
    "\n",
    "H = 200 # No of hidden nodes\n",
    "std=1e-6 # standard deviation (sigma)\n",
    "w1 = std*np.random.randn(Din, H) #Return a sample (or samples) from the “standard normal” distribution.\n",
    "w2 = std*np.random.randn(H, K)\n",
    "b1 = np.zeros(H) # creating array of zeros\n",
    "b2 = np.zeros(K)\n",
    "print(\"w1:\", w1.shape)\n",
    "print(\"w2:\", w2.shape)\n",
    "print(\"b1:\", b1.shape)\n",
    "print(\"b2:\", b2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "guided-acquisition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 3| loss 0.779884| training accuracy 77.601320| testing accuracy 77.255000\n",
      "iteration 1 / 3| loss 0.777098| training accuracy 77.984597| testing accuracy 77.535000\n",
      "iteration 2 / 3| loss 0.784402| training accuracy 77.596719| testing accuracy 77.137000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 49990 # for gradient descent optimization batch size is equal to number of training set\n",
    "\n",
    "iterations = 3 # epochs\n",
    "lr = 1.4e-2 # the learning rate alpha\n",
    "lr_decay = 0.999\n",
    "reg = 5e-6 # the regularization constant\n",
    "loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "lr_history = []\n",
    "\n",
    "for t in range(iterations):\n",
    "    \n",
    "    for start in range(0, Ntr, batch_size):\n",
    "        batch_indices = np.random.choice(Ntr, batch_size)\n",
    "        x = x_train[batch_indices]\n",
    "        y = y_train[batch_indices]\n",
    "        \n",
    "        #forward\n",
    "        #----------------------------------------------for train set---------------------------------------------------------------------\n",
    "        h = 1.0/(1.0 + np.exp(-(x.dot(w1) + b1 ))) # create a activation function (sigmoid function)\n",
    "        y_pred = h.dot(w2) + b2 # create predictable output\n",
    "        #----------------------------------------------for test set----------------------------------------------------------------------\n",
    "        h_t = 1.0/(1.0 + np.exp(-(x_test.dot(w1) + b1 ))) # create a activation function for test data (sigmoid function)\n",
    "        y_pred_test = h_t.dot(w2) + b2 # create predictable output\n",
    "        #--------------------------------------------------------------------------------------------------------------------------------\n",
    "        loss = (1./(y_pred.shape[0]))*np.square(y_pred - y).sum() + reg*(np.sum(w2*w2) + np.sum(w1*w1)) # loss function with regularization term \n",
    "        \n",
    "    \n",
    "        # compute the accuracy as percentage\n",
    "        training_acc = 100*(1 - (1/((y_pred.shape[0])*K))*(np.abs(np.argmax(y,axis=1) - np.argmax(y_pred,axis=1))).sum())  \n",
    "        testing_acc = 100*(1 - (1/((y_pred_test.shape[0])*K))*(np.abs(np.argmax(y_test,axis=1) - np.argmax(y_pred_test,axis=1))).sum())\n",
    "        \n",
    "        \n",
    "        #backward\n",
    "        dy_pred = 1./batch_size*2.0*(y_pred - y) # partial derivatives w.r.t y_predicted\n",
    "        dw2 = h.T.dot(dy_pred) + reg*w2\n",
    "        db2 = dy_pred.sum(axis=0)\n",
    "        dh = dy_pred.dot(w2.T)\n",
    "        dw1 = x.T.dot(dh*h*(1-h)) + reg*w1\n",
    "        db1 = (dh*h*(1-h)).sum(axis=0)\n",
    "\n",
    "        # updating learning parameters\n",
    "        w1 -= lr*dw1\n",
    "        w2 -= lr*dw2\n",
    "        b1 -= lr*db1\n",
    "        b2 -= lr*db2\n",
    "        lr *= lr_decay\n",
    "        \n",
    "    train_acc_history.append(training_acc)\n",
    "    val_acc_history.append(testing_acc)\n",
    "    loss_history.append(loss)\n",
    "    lr_history.append(lr)\n",
    "    \n",
    "    if t % 1 == 0:\n",
    "            print(\"iteration %d / %d| loss %f| training accuracy %f| testing accuracy %f\" % (t, iterations, loss, training_acc, testing_acc))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-expense",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
