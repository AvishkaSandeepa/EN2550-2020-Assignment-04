{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "colored-composer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: (3072, 10)\n",
      "b: (10,)\n",
      "iteration 0 / 300: loss 1.000000\n",
      "iteration 10 / 300: loss 1.000000\n",
      "iteration 20 / 300: loss 1.000000\n",
      "iteration 30 / 300: loss 1.000000\n",
      "iteration 40 / 300: loss 1.000000\n",
      "iteration 50 / 300: loss 1.000000\n",
      "iteration 60 / 300: loss 1.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-fc2f65f4b5d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mlos_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "K = len(np.unique(y_train)) # Return the unique elements of a tratining output set and take it length as Classes\n",
    "Ntr = x_train.shape[0] # number of training examples\n",
    "Nte = x_test.shape[0] # number of testing examples\n",
    "Din = 3072 # By CIFAR10 data set with 32 x 32 x 3 color images\n",
    "\n",
    "# Normalize pixel values: Image data preprocessing\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "mean_image = np.mean(x_train, axis=0) # axis=0: mean of a column; Mean of each pixel\n",
    "x_train = x_train - mean_image\n",
    "x_test = x_test - mean_image\n",
    "\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=K) # This function returns a matrix of binary values (either ‘1’ or ‘0’). It has number of rows equal to the length of the input vector and number of columns equal to the number of classes.\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=K)\n",
    "x_train = np.reshape(x_train,(Ntr,Din)).astype('float32') # reshape the data set\n",
    "x_test = np.reshape(x_test,(Nte,Din)).astype('float32')\n",
    "\n",
    "std=1e-6 # standard deviation (sigma)\n",
    "w = std*np.random.randn(Din, K) #Return a sample (or samples) from the “standard normal” distribution.\n",
    "b = np.zeros(K) # creating array of zeros for bias vector\n",
    "\n",
    "\n",
    "print(\"w:\", w.shape)\n",
    "print(\"b:\", b.shape)\n",
    "batch_size = Ntr # for gradient descent optimization batch size is equal to number of training set\n",
    "\n",
    "iterations = 300 # epochs\n",
    "lr = 1.4e-2 # the learning rate alpha\n",
    "lr_decay = 0.999\n",
    "reg = 5e-6 # the regularization constant\n",
    "los_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "for t in range(iterations):\n",
    "    indices = np.arange(Ntr)\n",
    "    rng.shuffle(indices)\n",
    "    x = x_train[indices]\n",
    "    y = y_train[indices]\n",
    "\n",
    "    y_pred = x.dot(w) + b\n",
    "    loss = (1/batch_size)*(np.square(y_pred - y)).sum() + reg*(np.sum(w*w))\n",
    "    los_history.append(loss)\n",
    "\n",
    "    if (t % 10 == 0):\n",
    "        print('iteration %d / %d: loss %f' % (t, iterations, loss)) \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-comedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "print('x_train:', x_train.shape)\n",
    "K = len(np.unique(y_train)) # Return the unique elements of a tratining output set and take it length as Classes\n",
    "Ntr = x_train.shape[0] # number of training examples\n",
    "Nte = x_test.shape[0] # number of testing examples\n",
    "Din = 3072 # By CIFAR10 data set with 32 x 32 x 3 color images\n",
    "\n",
    "x_train = x_train[range(Ntr), :]\n",
    "x_test = x_test[range(Nte), :]\n",
    "y_train = y_train[range(Ntr)]\n",
    "y_test = y_test[range(Nte)]\n",
    "\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=K) # This function returns a matrix of binary values (either ‘1’ or ‘0’). It has number of rows equal to the length of the input vector and number of columns equal to the number of classes.\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=K)\n",
    "x_train = np.reshape(x_train,(Ntr,Din)).astype('float32') # reshape the data set\n",
    "x_test = np.reshape(x_test,(Nte,Din)).astype('float32')\n",
    "\n",
    "#x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "mean_image = np.mean(x_train, axis=0)\n",
    "x_train = x_train - mean_image\n",
    "x_test = x_test - mean_image\n",
    "\n",
    "H = 200 # No of hidden nodes\n",
    "std=1e-6 # standard deviation (sigma)\n",
    "w1 = std*np.random.randn(Din, H) #Return a sample (or samples) from the “standard normal” distribution.\n",
    "w2 = std*np.random.randn(H, K)\n",
    "b1 = np.zeros(H) # creating array of zeros\n",
    "b2 = np.zeros(K)\n",
    "print(\"w1:\", w1.shape)\n",
    "print(\"w2:\", w2.shape)\n",
    "print(\"b1:\", b1.shape)\n",
    "print(\"b2:\", b2.shape)\n",
    "batch_size = Ntr # for gradient descent optimization batch size is equal to number of training set\n",
    "\n",
    "iterations = round(Ntr/batch_size)*300 # epochs\n",
    "lr = 1.4e-2 # the learning rate alpha\n",
    "lr_decay = 0.999\n",
    "reg = 5e-6 # the regularization constant\n",
    "loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "val = []\n",
    "\n",
    "for t in range(iterations):\n",
    "    batch_indices = np.random.choice(Ntr, batch_size)\n",
    "    x = x_train[batch_indices]\n",
    "    y = y_train[batch_indices]\n",
    "    h = 1.0/(1.0 + np.exp(-(x.dot(w1) + b1 ))) # create a activation function (sigmoid function)\n",
    "    y_pred = h.dot(w2) + b2 # create predictable output\n",
    "    loss = (1./batch_size)*np.square(y_pred - y).sum() + reg*(np.sum(w2*w2) + np.sum(w1*w1)) # loss function with regularization term \n",
    "    loss_history.append(loss)\n",
    "    if t % 10 == 0:\n",
    "        print('iteration %d / %d: loss %f' % (t, iterations, loss))\n",
    "\n",
    "    dy_pred = 1./batch_size*2.0*(y_pred - y)\n",
    "    dw2 = h.T.dot(dy_pred) + reg*w2\n",
    "    db2 = dy_pred.sum(axis=0)\n",
    "    dh = dy_pred.dot(w2.T)\n",
    "    dw1 = x.T.dot(dh*h*(1-h)) + reg*w1\n",
    "    db1 = (dh*h*(1-h)).sum(axis=0)\n",
    "\n",
    "    w1 -= lr*dw1\n",
    "    w2 -= lr*dw2\n",
    "    b1 -= lr*db1\n",
    "    b2 -= lr*db2\n",
    "    lr *= lr_decay\n",
    "\n",
    "   # train_acc_history.append(getAccuracy(y_pred, y_train))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
